\chapter{Конструкторский раздел}
\todo[inline]{25 – 30 страниц}
\todo[inline]{написать что такое коллекции}
%
\section{Общие сведения}

Для разработки программной реализации необходимо разбить весь процесс на этапы. При разбиении следует руководствоваться теми или иными, уже существующими, решениями для хранения и обработки информации.

В сети Интернет документы хранятся в виде html файлов.
Для обучения модели необходимо получить данные в виде мешков слов.
Кроме того, из-за большого объема данных необходимо разделить предварительную обработку каждого документа и последующий сбор обработанных данных в общую коллекцию для обучения.

Отдельным пунктом была рассмотрена структура данных, чтобы при выборе средств реализации можно было использовать эту информацию. 

Процесс создания тематической модели разбивается на следующие этапы:

\begin{itemize}
    \item Сбор данных
    \item Обработка данных
    \item Обучение модели
    \item Использование модели
    \item Оценка модели
\end{itemize}


%
\section{Структура данных}

\todo[inline] {добавить высокоуровневый тип данных, ограничения}

Очевидно, что для работы решения необходимо хранить коллекцию новостей, где о каждом документе известны: тема новости, текст новости, ссылка на html файл новости в сети Интернет.

Так как обрабатываются данные по документам, будет удобно иметь данные в обработанном виде радом с сырыми, чтобы иметь возможность обрабатывать коллекцию по частям.

При описании структуры данных желательно предоставить возможность обновлять данные, так как со временем html документы на выбранном ресурсе могут меняться. Для этого необходимо хранить дату сохранения документов.

Кроме того процесс обработки данных так же может быть усовершенствован или изменен. Следовательно так же необходимо хранить дату обработки данных.

Так как все данные текстовые и однородные, для хранения выбрана таблица в базе данных со следующими полями: 

\begin{itemize}
    \item Тема новости
    \item Текст новости 
    \item Ссылка на html файл новости в сети Интернет
    \item Обработанный текст новости
    \item Дата сохранения документа
    \item Дата обработки данных
\end{itemize}

~\

Для организации сохранения всех новостей с выбранного ресурса необходимо отслеживать на какие страницы ресурса ведут уже обработанные страницы. Для этого создается еще одна таблица с данными: какая ссылка, на какую другу ссылку ведет. То есть создается таблица со следующими полями: 

\begin{itemize}
    \item Ссылка родитель
    \item Ссылка ребенок 
\end{itemize}

%
\section{Сбор данных}
\todo[inline]{как будем извлекать данные (без кода пока)}
\todo[inline]{Мой написанный код для парсинга}
\todo[inline]{Уже предварительно собранные открытые данные}
\todo[inline]{https://newspaper.readthedocs.io/en/latest/ - возможный инструмент для парсинга}
\todo[inline]{25 500 новостей (там суммарно 9 000 000 слов - я посчитал) за все время существования media.zone (я сам написал парсер, могу его же натравить на любой другой новостной ресурс) - уже скачены и лежат на моем компьютере}
\todo[inline]{\href{http://www.statmt.org/wmt15/translation-task.html}{statmt.org - это не совсем подходит нам, тут новости короткие совсем. Но тоже скачал на всякий случай поиграться - тут суммарно 8,4 гигабайта чистого текста - уже скачены и лежат на моем компьютере}}
\todo[inline]{\href{https://webhose.io/free-datasets/russian-news-articles/}{webhose.io - 290 000 новостей - уже скачены и лежат на моем компьютере}}
\todo[inline]{Можно сделать сервис на РИА новости}
\todo[inline]{Можно сделать сервис на агрегаторы новостей}

В аналитическом разделе были выделены несколько типов данных: 

\begin{itemize}
    \item Предварительно подготовленные массивы новостей 
    \item Новостные сайты
    \item Новостные агригаторы
\end{itemize}

~\

Рассмотрим их детальнее. 

%
\subsubsection{Предварительно подготовленные массивы новостей}

Обычно в таких массивах данных текст новостей и их заголовки уже очищены от форматирования и переносов, опечатки исправлены, а так же удалена нетекстовая информация. При этом остается проблема слишком коротких текстов, слова в новостях не приведены к нормальной форме, не выделены словосочетания, много слишком часто используемых слов и слишком редко используемых слов. Также отдельной проблемой является то, что каждый такой массив данных оформлен по-своему, поэтому для работы с ним необходимо писать код, преобразующий коллекцию в удобный для модели формат.

Так как часть обработки уже выполнена, получить такой массив данных, предпочтительнее, чем добывать данные из сети Интернет. Но стоит учесть, что найти такие массивы данных достаточно сложно. Необходимо опрашивать специалистов в этой области, изучать платформы сообществ по обработке естественного языка, анализировать архивы конференций.

%
\subsubsection{Новостные сайты и агригаторы}

У данных, хранящихся в сети Интернет, существует большое количество недостатков: они не обработаны, текст хранится в перемешку с html кодом, содержит опечатки. Также из-за неорганизованности владельцев новостных сайтов, зачастую важные для последующего анализа данные (например, дата публикации, имя документа и т.д.) хранятся в разном виде за разные периоды времени, и поэтому сложно их сложно извлечь. 

С другой стороны, такой подход предоставляет практически безграничные возможности выбора тематики для последующего анализа.

Для извлечения таких данных необходим специальный софт, который анализирует указанный интернет ресурс, а так же все ссылки, на которые ведут уже скаченные страницы. Отдельно стоит отметить,и что часть ссылок зачастую на новостных сайтах появляются динамически, после того, как посетитель сайта нажимает специальную кнопку или перематывает страницу до конца.

Также учитывая технические ограничения автора работы и то, что документов на выбранном ресурсе может быть много, необходимо, чтобы процесс анализа и сохранения новостей можно было остановить в любой момент и в последствии продолжить с места остановки.

Так как данная задача довольно распространена существует библиотеки, частично или полностью решающие проблему получения данных. Однако, часто данные на сайтах хранятся в таком виде, что приходится модифицировать существующие решения. 

%
\section{Обработка данных}
\todo[inline]{рассказать, что такое лемматизация}


После того, как получены сырые данные, перед началом обучения модели, данные необходимо подготовить. Подготовка данных разбивается на два этапа:

\begin{itemize}
    \item Обработка документа (новости)
    \item Формирование коллекции в формате удобным для модели
\end{itemize}

%
\subsubsection{Обработка документа (новости)}

В рамках этого этапа подготовки данных производится обработка по документам. В связи с техническими ограничениями необходим хранить дату обработки, чтобы иметь возможность при изменении алгоритма выполнить процесс подготовки текста повторно. Кроме того, так же, как и в случае с сохранением страницы сети Интернет, из-за того, что данных много, необходимо реализовать возможность подготовки новостей коллекции по частям, останавливая и запуская процесс в любой момент времени.

Подготовку данных по документам можно разбить на следующие этапы:

\begin{itemize}
    \item\textbf{Очистка от форматирования и переносов.} В сыром виде текст новости часто перемешан с html кодом, специальными символами pdf файлов, часть слов разделены дефисов для переноса на новую строку.
    \item\textbf{Исправление опечаток.} Журналисты и редакторы могут не уследить за орфографической ошибкой и обучаемая модель воспримет слово с ошибкой, как отдельное редкое слово в коллекции.
    \item\textbf{Удаление нетекстовой информации.} Например, рисунков, графиков, таблиц.
    \item\textbf{Приведение слов к нормальной форме.} Для английского языка используется \textbf{стемминг}. Для исследования данной работы лучше подходит \textbf{лемматизация}, так как новости на русском языке.
    \item\textbf{Выделение словосочетаний.} По умолчанию модель воспринимает каждое слово в тексте новости, как отдельный термин. При выделении словосочетаний появляется возможность, обучая модель относиться к ним как к цельным терминам.
    \item\textbf{Удаление часто используемых слов.} Часто используемые слова встречаются в большом количестве тем, и их наличие в документе не может стать признаком того, какие именно темы затрагиваются в новости. 
    \item\textbf{Удаление редко используемых слов.} Редко используемые слова (обычно меньше десяти раз за коллекцию) также не несут собой обычно никакой информации о принадлежности документа к той или иной теме.
\end{itemize}
%
\subsubsection{Формирование коллекции в формате удобным для модели}

%
\section{Обучение модели}
\todo[inline]{разработка метода}
\todo[inline]{Базовый алгоритм: ARTM (bigartm.readthedocs.io)}
\todo[inline]{Предобработка текста: лемматизация, удаление стоп-слов, ngrams}
\todo[inline]{Используем модальности (дата публикации, ссылки на другие документы, авторы)}
\todo[inline]{Используем производные от статьи данные по различным алгоритмам (записываем в модальности) - алгоритмы еще не выбраны}
\todo[inline]{IDEF0 метода}

%
\section{Использование модели}
\todo[inline]{Можно попробовать обучаться на месяце/неделе/дне (и это в теории можно вынести в экперимент) и выдавать как меняются темы}
\todo[inline]{решить иерархически ли хотим строить темы или многое ко многим}

%
\section{Оценка модели}
\todo[inline]{как будем оценивать (без кода)}
\todo[inline]{Разбиение на 2 части и замеры разницы оценки - устойчивость - Через предложение разбивать статью можно попробовать}
\todo[inline]{Толока - описание теста - выбрать лишнее слово, подумать что еще можно}

%
\section{Требования к программе}

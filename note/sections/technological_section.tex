\chapter{Технологический раздел}
\todo[inline]{20 - 25 страниц}
\todo[inline]{обоснованный выбор средств программной реализации}
\todo[inline]{описание основных (нетривиальных) моментов разработки}

%
\section{Выбор средств программной реализации и разработка}

%%
\subsection{Создание базы данных}

Для данной работы рассматривается несколько самых известных реализаций реляционных баз данных:

\begin{itemize}
    \item MySQL
    \item SQLite
    \item PostrgreSQL
\end{itemize}

%%%
\subsubsection{MySQL}

Решение от компании Oracle. Очень популярное и мощное решение для малых и средних приложений, распространяемое под лицензией \href{https://ru.wikipedia.org/wiki/GNU_General_Public_License}{GNU General Public License}. Преимущества этого решения - популярность и богатый функционал. Из недостатков можно отметить требовательность к ПО и относительно медленная разработка.

%%%
\subsubsection{SQLite}

Компактная встраиваемая СУБД. Движок SQLite представляет собой библиотеку, а не отдельно работающий процесс. При работе с этой СУБД обращения происходят напрямую к файлам. Среди недостатков можно отметить небольшое количество типов данных, доступных по умолчанию, отсутствие системы пользователей. Среди преимуществ хранение всей базы одним файлом.

%%%
\subsubsection{PostrgreSQL}

Самое профессиональное из всех трех рассмотренных решений. Обладает богатым функционалом. PostrgreSQL это не только реляционная СУБД, но также и объектно-ориентированная. К недостаткам можно отнести низкую производительность на простых операциях.

%%%
\subsubsection{Выбр СУБД}

Исходя из технических требований для этой работы выбор был остановлен на SQLite. Использование данного решения позволяет хранить все в одном файле и упрощает стартовую настройку решения. Ограниченность функционала и типов данных не будет проблемой в связи с простой структурой данных.

В качестве дополнительного функционала был реализован подсчет рейтинга страниц, который становится тем больше чем больше ссылок ведет на рассматриваемую страницу. Данный подход часто используется при сортировке страниц в поисковой выдаче. Эти данные могут пригодиться для процесса сохранения html-файлов. Можно модифицировать решение и в первую очередь скачивать страницы с наибольшим рейтингом.

%%
\subsection{Сбор данных}

В работе используется два источника данных: новостные сайты и агрегаторы и предварительно подготовленные открытые массивы новостей. Работа с агрегатором новостей ничем не отличается от работы с сайтом новостного агентства.

%%%
\subsubsection{Предварительно подготовленные массивы новостей}

Самое сложное в получении готовых массивов данных - найти их. Для того, что бы поработать с большим объемом информации были проанализированы переписки 

В сообществе Open Data Science были найдены ссылки на два массива данных:

\begin{itemize}
    \item \href{http://www.statmt.org/wmt15/translation-task.html}{statmt.org - это не совсем подходит нам, тут новости короткие совсем. Но тоже скачал на всякий случай поиграться - тут суммарно 8,4 гигабайта чистого текста - уже скачены и лежат на моем компьютере}
    \item \href{https://webhose.io/free-datasets/russian-news-articles/}{webhose.io - 290 000 новостей - уже скачены и лежат на моем компьютере}
\end{itemize}
~\

После посещения конференции "Диалог" стало понятно где найти еще три массива данных: 

\begin{itemize}
    \item Lenta.ru
    \item Россия сегодня
    \item РИА новости
\end{itemize}
~\

Было принято решение дальше работать именно с этими массивами данных.

%%%
\subsubsection{Новостные сайты и агрегаторы}

Для начала сбора данных необходимо убедиться, что в базе данных присутствуют все необходимые сущности и поля для скачивания. Поэтому в начале программы реализован анализ состояния базы и если база не соответствует требованиям программы для сбора html-страниц - программа создает нужные сущности и поля.

Существует множество библиотек для анализа html страниц. Было принято решение воспользоваться самой популярной из них - <<BeautifulSoup>>. Данная библиотека позволяет разобрать html файл на теги и производить операции по ним.

Так как на вход программа получает только корневую ссылку ресурса - необходимо, что бы все внутренние ссылки главной html страницы новостного ресурса так же добавлялись в список на проверку. Для того, что бы избежать смещения скаченных данных к определенной дате или теме - ссылки из списка запланированных на скачивание страниц должны выбираться случайным образом.

Кроме того часть новостей может скрываться за кнопками вида <<Показать еще>> и действиями пользователя (например перемотка страницы новостей). Для того, что бы выполнить требование, по которому программу сбора данных можно остановить в любой момент, что бы потом продолжить с того же места необходимо записывать в базу html-файл каждой обработанной страницы.

Для того, что бы пользователю было понятно, что процесс протекает нормально принято решение каждые 50 обработанных страниц выводить промежуточную статистику в терминал. При каждом сохранении новости записывается дата сохранения, что бы в последствии данные в базе можно был сравнивать с данными по ссылке и обновлять при необходимости.

%%
\subsection{Обработка данных}

Обработка данных разделена на два этапа: подокументная обработка и подготовка коллекции для обучения модели.

%%%
\subsubsection{Обработка подокументно}

%%%
\subsubsection{Подготовка коллекции}

%%
\subsection{Обучение модели}

%%
\subsection{Использование модели}

%%%
\subsubsection{Оценка документа}

%%%
\subsubsection{Дообучение}

%%
\subsection{Оценка модели}

%
\section{Тестирование}
\todo[inline]{методики тестирования созданного программного обеспечения}

%
\section{Подготовка к запуску}
\todo[inline]{информация, необходимая для сборки и запуска разработанного программного обеспечения}
\chapter{Технологический раздел}
\todo[inline]{20 - 25 страниц}
\todo[inline]{обоснованный выбор средств программной реализации}
\todo[inline]{описание основных (нетривиальных) моментов разработки}

%
\section{Выбор средств программной реализации и разработка}

%%
\subsection{Выбор основного языка программирования}
\todo[inline]{Прописать сюда питон, сравнение и преимущетсва}

%%
\subsection{Создание базы данных}

Для данной работы рассматривается несколько самых известных реализаций реляционных баз данных:

\begin{itemize}
    \item MySQL
    \item SQLite
    \item PostrgreSQL
\end{itemize}

%%%
\subsubsection{MySQL}

Решение от компании Oracle. Очень популярное и мощное решение для малых и средних приложений, распространяемое под лицензией \href{https://ru.wikipedia.org/wiki/GNU_General_Public_License}{GNU General Public License}. Преимущества этого решения - популярность и богатый функционал. Из недостатков можно отметить требовательность к ПО и относительно медленная разработка.

%%%
\subsubsection{SQLite}

Компактная встраиваемая СУБД. Движок SQLite представляет собой библиотеку, а не отдельно работающий процесс. При работе с этой СУБД обращения происходят напрямую к файлам. Среди недостатков можно отметить небольшое количество типов данных, доступных по умолчанию, отсутствие системы пользователей. Среди преимуществ хранение всей базы одним файлом.

%%%
\subsubsection{PostrgreSQL}

Самое профессиональное из всех трех рассмотренных решений. Обладает богатым функционалом. PostrgreSQL это не только реляционная СУБД, но также и объектно-ориентированная. К недостаткам можно отнести низкую производительность на простых операциях.

%%%
\subsubsection{Выбр СУБД}

Исходя из технических требований для этой работы выбор был остановлен на SQLite. Использование данного решения позволяет хранить все в одном файле и упрощает стартовую настройку решения. Ограниченность функционала и типов данных не будет проблемой в связи с простой структурой данных.

В качестве дополнительного функционала был реализован подсчет рейтинга страниц, который становится тем больше чем больше ссылок ведет на рассматриваемую страницу. Данный подход часто используется при сортировке страниц в поисковой выдаче. Эти данные могут пригодиться для процесса сохранения html-файлов. Можно модифицировать решение и в первую очередь скачивать страницы с наибольшим рейтингом.

%%
\subsection{Сбор данных}

\todo[inline]{Мой написанный код для парсинга}
\todo[inline]{Уже предварительно собранные открытые данные}
\todo[inline]{https://newspaper.readthedocs.io/en/latest/ - возможный инструмент для парсинга}
\todo[inline]{25 500 новостей (там суммарно 9 000 000 слов - я посчитал) за все время существования media.zone (я сам написал парсер, могу его же натравить на любой другой новостной ресурс) - уже скачены и лежат на моем компьютере}
\todo[inline]{\href{http://www.statmt.org/wmt15/translation-task.html}{statmt.org - это не совсем подходит нам, тут новости короткие совсем. Но тоже скачал на всякий случай поиграться - тут суммарно 8,4 гигабайта чистого текста - уже скачены и лежат на моем компьютере}}
\todo[inline]{\href{https://webhose.io/free-datasets/russian-news-articles/}{webhose.io - 290 000 новостей - уже скачаны и лежат на моем компьютере}}
\todo[inline]{Можно сделать сервис на РИА новости}
\todo[inline]{Можно сделать сервис на агрегаторы новостей}

В работе используется два источника данных: новостные сайты и агрегаторы и предварительно подготовленные открытые массивы новостей. Работа с агрегатором новостей ничем не отличается от работы с сайтом новостного агентства.

%%%
\subsubsection{Предварительно подготовленные массивы новостей}

Самое сложное в получении готовых массивов данных - найти их. Для того, что бы поработать с большим объемом информации были проанализированы переписки 

В сообществе Open Data Science были найдены ссылки на два массива данных:

\begin{itemize}
    \item \href{http://www.statmt.org/wmt15/translation-task.html}{statmt.org - это не совсем подходит нам, тут новости короткие совсем. Но тоже скачал на всякий случай поиграться - тут суммарно 8,4 гигабайта чистого текста - уже скачены и лежат на моем компьютере}
    \item \href{https://webhose.io/free-datasets/russian-news-articles/}{webhose.io - 290 000 новостей - уже скачены и лежат на моем компьютере}
\end{itemize}
~\

После посещения конференции "Диалог" стало понятно где найти еще три массива данных: 

\begin{itemize}
    \item Lenta.ru
    \item Россия сегодня (РИА новости)
\end{itemize}
~\

Было принято решение дальше работать именно с этими массивами данных.

%%%
\subsubsection{Новостные сайты и агрегаторы}

Для начала сбора данных необходимо убедиться, что в базе данных присутствуют все необходимые сущности и поля для скачивания. Поэтому в начале программы реализован анализ состояния базы и если база не соответствует требованиям программы для сбора html-страниц - программа создает нужные сущности и поля.

Существует множество библиотек для анализа html страниц. Было принято решение воспользоваться самой популярной из них - <<BeautifulSoup>>. Данная библиотека позволяет разобрать html файл на теги и производить операции по ним.

Так как на вход программа получает только корневую ссылку ресурса - необходимо, что бы все внутренние ссылки главной html страницы новостного ресурса так же добавлялись в список на проверку. Для того, что бы избежать смещения скаченных данных к определенной дате или теме - ссылки из списка запланированных на скачивание страниц должны выбираться случайным образом.

Кроме того часть новостей может скрываться за кнопками вида <<Показать еще>> и действиями пользователя (например перемотка страницы новостей). Для того, что бы выполнить требование, по которому программу сбора данных можно остановить в любой момент, что бы потом продолжить с того же места необходимо записывать в базу html-файл каждой обработанной страницы.

Для того, что бы пользователю было понятно, что процесс протекает нормально принято решение каждые 50 обработанных страниц выводить промежуточную статистику в терминал. При каждом сохранении новости записывается дата сохранения, что бы в последствии данные в базе можно был сравнивать с данными по ссылке и обновлять при необходимости.

%%
\subsection{Обработка данных}
\todo[inline]{Добавить обрезание часто и редкоиспользуемых слов и посмотреть что там еще есть в списке}
\todo[inline]{Рассказать что такое формат vowpal wabbit}
\todo[inline]{Рассказать что такое батчи}

Обработка данных разделена на два этапа: подокументная обработка и подготовка коллекции для обучения модели. В обработке по документам необходимо из html файла получить мешок слов и сохранить его в базе в соответствующем поле. При подготовке коллекции к обучению необходимо собрать из базы и приготовить данные в том виде, в котором требует реализация выбранного алгоритма (выбор реализации алгоритма приведен ниже).

%%%
\subsubsection{Обработка подокументно}

Обработка документа содержит следующие этапы:

\begin{itemize}
    \item преобразование html кода в текст,
    \item леммирование слов,
    \item преобразование текста в формат vowpal wabbit
\end{itemize}
~\

При преобразовании html кода в текст используется рассмотренная выше популярная библиотека <<BeautifulSoup>>. Исследователем устанавливается какие теги новостной ресурс использует для хранения заголовка и текста статьи. Программа настраивается в соответствии с этим выявленным шаблоном. Все что находится внутри настроенных тегов очищается от html разметки и сохраняется в виде текста в базу с документами в соответствующие записи. Этот процесс вынесен в отдельную процедуру и так же как и процесс сохранения страниц может быть в любой момент остановлен и в последствии запущен снова.

После того как получены данные в виде текста на русском языке производится леммирование слов и преобразование в формат vowpal wabbit. В процессе удаляются все слова на английском языке, как не несущие большой значимости для модели. Слова, прошедшие леммирование сохраняются в соответствующее поле в базе через пробел.

%%%
\subsubsection{Подготовка коллекции}

Подготовка коллекции содержит следующие этапы:

\begin{itemize}
    \item выгрузка из базы документов в формате vowpal wabbit в текстовый файл
    \item преобразование текстового файла в формате vowpal wabbit в батчи
\end{itemize}
~\

Перед следующим этапом необходимо выгрузить все необходимые для обучения документы, прошедшие подокументную обработку в отдельный текстовый файл в формате vowpal wabbit. После чего этот файл преобразуется в батчи методом класса ARTM, встроенным в выбранную реализацию алгоритма ARTM (рассмотрена ниже).

%%
\subsection{Обучение модели}
\todo[inline]{Добавить выбор реализации - BigARTM}
\todo[inline]{Прописать какие именно регуляризаторы и в какой последовательности были добавлены и какие коэфициенты получились}

Согласно выбранному алгоритму сначала модель обучается на подготовленных данных без регуляризаторов (как в рассматриваемом варианте алгоритма PLSA) до того момента как сойдется матрица $\Phi$. Это будет означать, что слова достаточно хорошо и однозначно распределились по темам и осталась только задача тематизирования документов.

После этого в модель добавляются регуляризаторы по одному. Добавляя регуляризатор исследователь подбирает параметры регуляризатора, что бы \todo{}.

%%
\subsection{Использование модели}

Когда модель обучена ее можно сохранить методом \todo{}, встроенным в реализацию BigARTM для последующей загрузки.
\todo[inline]{Добавить пример}

%%%
\subsubsection{Оценка документа}

Новый поступивший документ (например новую написанную новость на сайте) можно тематизировать обученной моделью методом \todo{}.
\todo[inline]{Добавить пример}

%%%
\subsubsection{Дообучение}
Дообучить модель можно методом \todo{}.
\todo[inline]{Добавить пример}

%%
\subsection{Оценка модели}
\todo[inline]{Добавить сюда примеры из выбранных оценок выше}

%
\section{Тестирование}
\todo[inline]{методики тестирования созданного программного обеспечения, примеры}

%
\section{Подготовка к запуску}
\todo[inline]{информация, необходимая для сборки и запуска разработанного программного обеспечения}